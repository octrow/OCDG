# OCDG: Old Commit Description Generator

AI-powered Git commit message regeneration using LLMs.

## Purpose

Rewrites Git commit history with improved commit messages generated by LLMs (Llama3, OpenAI, Groq, Replicate, Ollama).

## Core Workflow

1. Clone/scan repository
2. Extract commit diffs
3. Generate improved messages via LLM
4. Rewrite commit history
5. Optional force-push

## Requirements

- Python 3.11+
- Poetry
- Git repository
- API keys for chosen LLM provider

## Installation

```bash
git clone https://github.com/octrow/OCDG.git
cd OCDG
python3 -m venv .venv
source .venv/bin/activate
poetry install
poetry shell
```

## Configuration

Copy `.env.example` to `.env`:

```bash
cp .env.example .env
```

Set required API keys:
- `NVIDIA_API_KEY` for OpenAI client
- `GROQ_API_KEY` for Groq client
- `REPLICATE_API_TOKEN` for Replicate client
- Ollama: no API key required (local)

For Ollama: Install from https://ollama.com/

## Usage

```bash
python main.py <repo_path> [-b <backup_dir>] [-l <llm_choice>] [-m <model>] [-f] [-r]
```

### Arguments

- `repo_path`: Local path or remote URL
- `-b`: Custom backup directory
- `-l`: LLM provider (`ollama`|`openai`|`groq`|`replicate`), default: `ollama`
- `-m`: Model name (provider-specific)
- `-f`: Force push after rewrite
- `-r`: Restore from backup

### Examples

```bash
# Local repo with Ollama (default)
python main.py /path/to/repo

# Remote repo with Groq
python main.py https://github.com/user/repo -l groq -m llama3-70b-8192

# With force push
python main.py /path/to/repo -l openai -m meta/llama3-70b-instruct -f

# Restore backup
python main.py /path/to/repo -r
```

## Docker

### Docker Build

```bash
docker build -t ocdg .
docker run --rm -v $(pwd)/repos:/app/repos --env-file .env ocdg /app/repos/your-repo
```

### Docker Compose

```bash
# Copy and configure .env
cp .env.example .env

# Run with cloud LLM (OpenAI, Groq, Replicate)
docker-compose run --rm ocdg /app/repos/your-repo -l groq

# Run with local Ollama
docker-compose --profile local-llm up -d ollama
docker-compose run --rm ocdg /app/repos/your-repo -l ollama
```

## Features

- Async concurrent processing
- Automatic Git backup/restore
- Exponential backoff retry logic (3 retries, 1s→2s→4s)
- Intelligent diff chunking for large commits
- JSON schema validation for LLM responses
- Multi-provider LLM support
- Configurable ignore patterns for binaries/dependencies
- Docker and Docker Compose support

## Architecture

### Components

- `main.py`: Orchestration, diff analysis, commit rewriting
- `clients/`: LLM client implementations
  - `base_client.py`: Abstract interface
  - `ollama_client.py`: Ollama async client
  - `openai_client.py`: NVIDIA/OpenAI async client
  - `groq_client.py`: Groq async client
  - `replicate_client.py`: Replicate async client
- `config.py`: Environment config, ignore patterns
- `retry_utils.py`: Exponential backoff retry decorator
- `test_ocdg.py`: Unit tests

### Classes

- `GitAnalyzer`: Git operations wrapper
- `Commit`: Single commit representation
- `CommitHistory`: Commit collection manager
- `RepositoryUpdater`: Safe commit rewriting with backup

## Configuration Files

### config.py

Modify `IGNORED_SECTION_PATTERNS` and `IGNORED_LINE_PATTERNS` to exclude paths/files from diff analysis.

Default ignored:
- `venv/`, `.idea/`, `node_modules/`, `__pycache__/`
- Binary files (images, executables, archives)
- Lock files (package-lock.json, poetry.lock)
- Log files, cache files

## Safety

- Creates refs backup before modifications
- User confirmation before rewriting
- Automatic restore on errors
- Force-push requires explicit `-f` flag

## Retry Logic

API failures automatically retry with exponential backoff:
- Max retries: 3
- Initial delay: 1s
- Backoff: 1s → 2s → 4s (max 60s)
- Handles: Rate limits, API errors, connection failures

## Limitations

- Beta stage
- Interactive rebase may fail in non-TTY environments
- Large repositories: memory-intensive
- No incremental processing

## Contributing

See CONTRIBUTING.md for development guidelines.

## License

MIT

## Author

octrow <octrow@yandex.ru>
